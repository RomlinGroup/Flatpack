[
  {
    "type": "python",
    "disabled": false,
    "code": "import sys\n\nprint(\"Building llama-cpp\")\nprint(f\"Python version: {sys.version}\")"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# llama.cpp\n# License: MIT\n\nif [ ! -d \"llama.cpp\" ]; then\n  git clone https://github.com/ggerganov/llama.cpp\n\n  cd llama.cpp\n\n  cmake -B build\n  cmake --build build --config Release\nelse\n  cd llama.cpp\nfi"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# google/gemma-2-2b-it\n# License: Gemma license\n# https://ai.google.dev/gemma/terms\n\nMODEL_FILE=\"models/gemma-2-2b-it-Q4_K_M.gguf\"\nURL=\"https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf\"\n\nif [ ! -f \"$MODEL_FILE\" ]; then\n    echo \"Downloading $MODEL_FILE...\"\n    wget \"$URL\" -O \"$MODEL_FILE\"\n    if [ $? -eq 0 ]; then\n        echo \"Download completed successfully\"\n    else\n        echo \"Download failed\"\n        rm -f \"$MODEL_FILE\"\n    fi\nelse\n    echo \"$MODEL_FILE already exists, skipping download\"\nfi"
  },
  {
    "type": "bash",
    "disabled": true,
    "code": "if [ -d \"vector\" ]; then\n    echo \"Directory 'vector' already exists. Deleting it now.\"\n    rm -rf \"vector\"\n    echo \"Directory 'vector' has been deleted.\"\nfi\n\necho \"Creating 'vector' directory now.\"\n\nmkdir \"vector\"\n\necho \"Directory 'vector' created successfully.\""
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "../bin/pip install -U kokoro-onnx\n[ ! -f ../kokoro-v0_19.onnx ] && wget -O ../kokoro-v0_19.onnx https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/kokoro-v0_19.onnx\n[ ! -f ../voices.json ] && wget -O ../voices.json https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/voices.json"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "import soundfile as sf\nimport subprocess\n\nfrom kokoro_onnx import Kokoro\n\nkokoro = Kokoro('kokoro-v0_19.onnx', 'voices.json')\n\nwhile True:\n    print(\"Enter your prompt (or 'quit' to exit):\")\n    user_input = input()\n   \n    if user_input.lower() == 'quit':\n        break\n\n    cmd = [\n        \"./llama.cpp/build/bin/llama-cli\",\n        \"-m\", \n        \"./llama.cpp/models/gemma-2-2b-it-Q4_K_M.gguf\",\n        \"--temp\",\n        \"0.7\",\n        \"--prompt\",\n        user_input,\n        \"--no-display-prompt\"\n    ]\n\n    print(\"\\nGenerating response...\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    generated_text = result.stdout.strip()\n    print(\"\\nResponse:\", generated_text)\n\n    with open('output/output.txt', 'w', encoding='utf-8') as f:\n        f.write(generated_text)\n\n    samples, sample_rate = kokoro.create(\n        generated_text,\n        voice='af_bella',\n        speed=1.0,\n        lang='en-us'\n    )\n    \n    sf.write('output/output.wav', samples, sample_rate)\n    print('\\nSaved response to output.txt and audio to output.wav')"
  }
]