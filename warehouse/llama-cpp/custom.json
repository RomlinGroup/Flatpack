[
  {
    "type": "bash",
    "disabled": false,
    "code": "# llama-cpp-python\n# License: MIT\n\n./bin/pip install llama-cpp-python \\\n    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# google/gemma-2-2b-it\n# License: Gemma license\n# https://ai.google.dev/gemma/terms\n\nMODEL_FILE=\"gemma-2-2b-it-Q4_K_M.gguf\"\nURL=\"https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf\"\n\nif [ ! -f \"$MODEL_FILE\" ]; then\n    echo \"Downloading $MODEL_FILE...\"\n    wget \"$URL\" -O \"$MODEL_FILE\"\n    if [ $? -eq 0 ]; then\n        echo \"Download completed successfully\"\n    else\n        echo \"Download failed\"\n        rm -f \"$MODEL_FILE\"\n    fi\nelse\n    echo \"$MODEL_FILE already exists, skipping download\"\nfi"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "if [ -d \"vector\" ]; then\n    echo \"Directory 'vector' already exists. Deleting it now.\"\n    rm -rf \"vector\"\n    echo \"Directory 'vector' has been deleted.\"\nfi\n\necho \"Creating 'vector' directory now.\"\n\nmkdir \"vector\"\n\necho \"Directory 'vector' created successfully.\""
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "./bin/pip install \\\nonnxruntime==1.20.1 \\\nphonemizer==3.3.0 \\\npiper-phonemize-cross==1.2.1 \\\npiper-tts==1.2.0 --no-deps"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# Cori (medium)\n# License: Public domain\n# https://brycebeattie.com/files/tts/\n\nif [ ! -f cori-med.onnx ]; then\n    wget -nc -O cori-med.onnx \"https://sfo3.digitaloceanspaces.com/bkmdls/cori-med.onnx\"\nelse\n    echo \"cori-med.onnx already exists.\"\nfi\n\nif [ ! -f cori-med.onnx.json ]; then\n    wget -nc -O cori-med.onnx.json \"https://sfo3.digitaloceanspaces.com/bkmdls/cori-med.onnx.json\"\nelse\n    echo \"cori-med.onnx.json already exists.\"\nfi"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "import json\nimport os\nimport subprocess\nimport sys\nimport time\n\nfrom llama_cpp import Llama\nfrom typing import Any, Dict, List, Optional\n\nclass Colors:\n    BLUE = '\\033[1;34m'\n    BOLD = '\\033[1m'\n    CYAN = '\\033[1;36m'\n    GREEN = '\\033[1;32m'\n    NC = '\\033[0m'\n    YELLOW = '\\033[1;33m'\n\ndef get_memory(input_text: str, vector_dir: str = \"vector\") -> Optional[str]:\n    try:\n        result = subprocess.run([\n            'flatpack', 'vector', 'search-text', input_text,\n            '--data-dir', vector_dir,\n            '--json',\n            '--recency-weight', '1.0'\n        ], capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            memory = json.loads(result.stdout)\n            return '\\n'.join(item['text'] for item in reversed(memory))\n    except Exception as e:\n        print(f\"Error retrieving memory: {e}\", file=sys.stderr)\n    return None\n\ndef save_to_memory(input_text: str, response: str, vector_dir: str = \"vector\"):\n    try:\n        subprocess.run([\n            'flatpack', 'vector', 'add-texts',\n            f\"User: {input_text}\\nAssistant: {response}\",\n            '--data-dir', vector_dir\n        ], capture_output=True)\n    except Exception as e:\n        print(f\"Error saving to memory: {e}\", file=sys.stderr)\n\ndef generate_and_play_audio(text: str, piper_model: str = \"cori-med.onnx\", output_dir: str = \"audio_output\"):\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        \n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        output_wav = os.path.join(output_dir, \"output.wav\")\n        output_txt = os.path.join(output_dir, \"output.txt\")\n        \n        with open(output_txt, 'w', encoding='utf-8') as f:\n            f.write(text)\n        \n        print(f\"{Colors.CYAN}Generating audio to {output_wav}...{Colors.NC}\")\n        subprocess.run([\n            './bin/piper',\n            '--model', piper_model,\n            '--output-file', output_wav\n        ], input=text.encode(), check=True)\n        \n        print(f\"{Colors.CYAN}Audio saved to {output_wav}{Colors.NC}\")\n        print(f\"{Colors.CYAN}Text saved to {output_txt}{Colors.NC}\")\n        \n    except Exception as e:\n        print(f\"Error with audio/text processing: {e}\", file=sys.stderr)\n        if os.path.exists(output_wav):\n            os.remove(output_wav)\n        if os.path.exists(output_txt):\n            os.remove(output_txt)\n\nclass LlamaChatNotebook:\n    def __init__(\n        self,\n        model_path: str = \"gemma-2-2b-it-Q4_K_M.gguf\",\n        vector_dir: str = \"vector\",\n        piper_model: str = \"cori-med.onnx\",\n        n_ctx: int = 4096,\n        n_batch: int = 128\n    ):\n        print(f\"{Colors.CYAN}Loading model...{Colors.NC}\")\n        self.llm = Llama(\n            model_path=model_path,\n            n_ctx=n_ctx,\n            n_batch=n_batch,\n            verbose=False\n        )\n        self.vector_dir = vector_dir\n        self.piper_model = piper_model\n\n    def chat(\n        self, \n        user_input: str, \n        use_memory: bool = True,\n        generate_audio: bool = True,\n        temperature: float = 0.7,\n        top_p: float = 0.95,\n        repeat_penalty: float = 1.1\n    ) -> str:\n        memory = None\n        if use_memory:\n            print(f\"{Colors.CYAN}Checking memory...{Colors.NC}\")\n            memory = get_memory(user_input, self.vector_dir)\n            if memory:\n                print(f\"{Colors.BLUE}<memory>{Colors.NC}\")\n                print(f\"{Colors.GREEN}{memory}{Colors.NC}\")\n                print(f\"{Colors.BLUE}</memory>{Colors.NC}\")\n\n        print(f\"{Colors.CYAN}Generating response...{Colors.NC}\")\n        \n        prompt = user_input\n        if memory:\n            prompt = f\"Previous context: {memory}\\n\\nCurrent question: {user_input}\"\n        \n        formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n<end_of_turn>\\n<start_of_turn>model\"\n        \n        response = self.llm.create_completion(\n            prompt=formatted_prompt,\n            max_tokens=tokens,\n            temperature=temperature,\n            top_p=top_p,\n            repeat_penalty=repeat_penalty,\n            echo=False\n        )['choices'][0]['text']\n        \n        print(f\"{Colors.YELLOW}<assistant>{response}</assistant>{Colors.NC}\")\n\n        if use_memory and response != \"I do not know, please try again.\":\n            print(f\"{Colors.CYAN}Saving to memory...{Colors.NC}\")\n            save_to_memory(user_input, response, self.vector_dir)\n\n        if generate_audio:\n            generate_and_play_audio(response, self.piper_model)\n\n        return response"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "chat = LlamaChatNotebook(\n    model_path=\"gemma-2-2b-it-Q4_K_M.gguf\",\n    vector_dir=\"vector\",\n    piper_model=\"cori-med.onnx\"\n)\n\nprint(\"Starting interactive chat (type 'quit' to exit)\")\nprint(\"-\" * 50)\n\nwhile True:\n    user_input = input(\"\\nYou: \").strip()\n    \n    if user_input.lower() in ['exit', 'q', 'quit']:\n        print(\"\\nEnding chat session...\")\n        break\n        \n    if not user_input:\n        continue\n        \n    try:\n        response = chat.chat(\n            user_input,\n            use_memory=True,\n            generate_audio=True\n        )\n        \n    except Exception as e:\n        print(f\"\\nError occurred: {str(e)}\")\n        print(\"Please try again.\")"
  }
]