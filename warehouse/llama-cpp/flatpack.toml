# flatpack.toml
# WIP ({{current_date}})

version = "0.0.1"

[environment]
model_name = "llama-cpp"
python_version = "3.12"

[[port]]
external = 8080
internal = 80

[directories]
checkpoints = "checkpoints"
data = "data"
models = "models"
scripts = "scripts"

[packages]

[packages.unix]
build-essential = "*"
gcc = "*"
git = "*"
jq = "*"
python3-dev = "*"
python3-pip = "*"
sox = "*"
wget = "*"

[packages.python]
numpy = "2.1.3"
tiktoken = "0.8.0"
tqdm = "4.67.1"

[[git]]
from_source = "https://github.com/ggerganov/llama.cpp"
to_destination = "llama.cpp"
branch = "master"
requirements_file = "requirements.txt"
setup_commands = [
    "cmake -B build",
    "cmake --build build --config Release"
]

[[file]]
from_source = "https://raw.githubusercontent.com/RomlinGroup/Flatpack/main/warehouse/llama-cpp/custom.json"
to_destination = "custom.json"

[[file]]
from_source = "https://raw.githubusercontent.com/RomlinGroup/Flatpack/main/warehouse/llama-cpp/build.sh"
to_destination = "build.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/RomlinGroup/Flatpack/main/warehouse/llama-cpp/device.sh"
to_destination = "device.sh"

[[run]]
command = "chmod +x"
file = "build.sh"

[[run]]
command = "chmod +x"
file = "device.sh"