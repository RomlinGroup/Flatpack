# flatpack.toml
# WIP ({{current_date}})

version = "0.0.1"

[environment]
model_name = "llama-cpp"

[[port]]
external = 8080
internal = 80

[directories]
checkpoints = "checkpoints"
data = "data"
models = "models"
scripts = "scripts"

[packages]

[packages.unix]
build-essential = "*"
gcc = "*"
git = "*"
python3-dev = "*"
python3-pip = "*"
wget = "*"

[packages.python]
numpy = "1.25.2"
tiktoken = "0.6.0"
tqdm = "4.66.2"

[[git]]
from_source = "https://github.com/ggerganov/llama.cpp"
to_destination = "llama.cpp"
branch = "master"
setup_commands = [
    "make",
    "cd models && flatpack compress microsoft/Phi-3-mini-4k-instruct"
]

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/index.html"
to_destination = "index.html"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/build.sh"
to_destination = "build.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/custom.sh"
to_destination = "custom.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/device.sh"
to_destination = "device.sh"

[[run]]
command = "chmod +x"
file = "build.sh"

[[run]]
command = "chmod +x"
file = "custom.sh"

[[run]]
command = "chmod +x"
file = "device.sh"