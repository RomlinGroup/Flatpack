# flatpack.toml
# WIP ({{current_date}})

version = "0.0.1"

[environment]
model_name = "llama-cpp"

[[port]]
external = 8080
internal = 80

[directories]
checkpoints = "checkpoints"
data = "data"
models = "models"
scripts = "scripts"

[packages]

[packages.unix]
build-essential = "*"
gcc = "*"
git = "*"
python3-dev = "*"
python3-pip = "*"
wget = "*"

[packages.python]
numpy = "2.0.0"
tiktoken = "0.7.0"
tqdm = "4.66.4"

# [[git]]
# from_source = "{{git_source_url}}"
# to_destination = "{{git_destination_dir}}"
# branch = "{{branch_name}}"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/index.html"
to_destination = "index.html"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/build.sh"
to_destination = "build.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/custom.sh"
to_destination = "custom.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/device.sh"
to_destination = "device.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/llama-cpp/init.sh"
to_destination = "init.sh"

[[run]]
command = "chmod +x"
file = "build.sh"

[[run]]
command = "chmod +x"
file = "custom.sh"

[[run]]
command = "chmod +x"
file = "device.sh"

[[run]]
command = "chmod +x"
file = "init.sh"

[[cmd]]
command = "bash"
file = "build.sh"