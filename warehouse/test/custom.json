[
  {
    "type": "bash",
    "disabled": false,
    "code": "rm -f test_results.json && touch test_results.json"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# python_venv_test\nimport json\nimport os\nimport sys\n\nfrom datetime import datetime\n\nprint(\"Starting Python venv test...\")\nprint(f\"Current directory: {os.getcwd()}\")\n\ntry:\n   with open('test_results.json', 'r') as f:\n       last_score = 0\n       for line in f:\n           try:\n               result = json.loads(line.strip())\n               if 'score' in result:\n                   last_score = int(result['score'])\n           except:\n               continue\nexcept (FileNotFoundError, ValueError):\n   last_score = 0\nprint(f\"Previous score: {last_score}\")\n\nparent_dir = os.path.dirname(os.getcwd())\nhas_bin = os.path.exists(os.path.join(parent_dir, 'bin'))\nhas_lib = os.path.exists(os.path.join(parent_dir, 'lib'))\nhas_pyvenv = os.path.exists(os.path.join(parent_dir, 'pyvenv.cfg'))\n\nprint(f\"Parent directory: {parent_dir}\")\nprint(f\"Has bin folder: {has_bin}\")\nprint(f\"Has lib folder: {has_lib}\")\nprint(f\"Has pyvenv.cfg: {has_pyvenv}\")\n\nif has_bin and has_lib and has_pyvenv:\n   score = last_score + 1\n   status = \"success\"\n   print(\"Test PASSED ✓\")\n   print(f\"Score increased to: {score}\")\nelse:\n   score = 0\n   status = \"fail\"\n   print(\"Test FAILED ✗\")\n   print(f\"Score reset to: {score}\")\n   print(\"Missing venv folders in parent directory\")\n\nwith open('test_results.json', 'a') as f:\n   f.write(json.dumps({\n       \"score\": score,\n       \"status\": status,\n       \"source\": \"python_venv_test\",\n       \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n   }) + \"\\n\")\n\nprint(f\"Results saved to test_results.json\")\nprint(f\"Final status: {status}\")"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# bash_variable_test (1)\nbash_variable_test=\"Hello, World!\""
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# bash_variable_test (2)\necho \"Starting test...\"\necho \"Current directory: $(pwd)\"\necho \"Testing bash_variable_test equals 'Hello, World!'\"\n\ncurrent_score=$(grep -o '\"score\": [0-9]*' test_results.json 2>/dev/null | tail -n1 | grep -o '[0-9]*' || echo 0)\necho \"Current score loaded: $current_score\"\n\n[ \"$bash_variable_test\" = \"Hello, World!\" ] && \\\n{ score=$((current_score + 1)); status=\"success\"; echo \"Test PASSED ✓\"; echo \"Score increased to: $score\"; } || \\\n{ score=0; status=\"fail\"; echo \"Test FAILED ✗\"; echo \"Score reset to: $score\"; } && \\\necho \"{\\\"score\\\": $score, \\\"status\\\": \\\"$status\\\", \\\"source\\\": \\\"bash_variable_test\\\", \\\"timestamp\\\": \\\"$(date \"+%Y-%m-%d %H:%M:%S\")\\\"}\" >> test_results.json && \\\necho \"Results saved to test_results.json\" && \\\necho \"Final status: $status\""
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# python_variable_test (1)\npython_variable_test = \"Hello, World!\""
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# python_variable_test (2)\nimport json\nimport os\n\nfrom datetime import datetime\n\nprint(\"Starting test...\")\nprint(f\"Current directory: {os.getcwd()}\")\nprint(f\"Testing python_variable_test equals 'Hello, World!'\")\n\ntry:\n    with open('test_results.json', 'r') as f:\n        for line in f:\n            if '\"score\":' in line:\n                last_score = int(line.split(':')[1].split(',')[0].strip())\nexcept (FileNotFoundError, ValueError):\n    last_score = 0\nprint(f\"Previous score: {last_score}\")\n\nif python_variable_test == \"Hello, World!\":\n    score = last_score + 1\n    status = \"success\"\n    print(\"Test PASSED ✓\")\n    print(f\"Score increased to: {score}\")\nelse:\n    score = 0\n    status = \"fail\"\n    print(\"Test FAILED ✗\")\n    print(f\"Score reset to: {score}\")\n\nwith open('test_results.json', 'a') as f:\n    f.write(json.dumps({\n        \"score\": score,\n        \"status\": status,\n        \"source\": \"python_variable_test\",\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }) + \"\\n\")\n\nprint(f\"Results appended to test_results.json\")\nprint(f\"Final status: {status}\")"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "../bin/pip install pexpect"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# python_input_test\nimport json\nimport os\nimport pexpect\nimport tempfile\n\nfrom datetime import datetime\n\nprint(\"Starting Python input test...\")\nprint(f\"Current directory: {os.getcwd()}\")\n\ntest_input = \"Hello, World!\"\nprint(f\"Testing input equals '{test_input}'\")\n\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n   f.write('value = input(\"Enter value: \")\\nprint(f\"Received: {value}\")\\n')\n   test_file = f.name\n\ntry:\n   try:\n       with open('test_results.json', 'r') as f:\n           last_score = 0\n           for line in f:\n               try:\n                   result = json.loads(line.strip())\n                   if 'score' in result:\n                       last_score = int(result['score'])\n               except:\n                   continue\n   except (FileNotFoundError, ValueError):\n       last_score = 0\n   print(f\"Previous score: {last_score}\")\n\n   child = pexpect.spawn(f'python3 {test_file}')\n   child.expect('Enter value: ')\n   child.sendline(test_input)\n   child.expect('Received: (.*)\\r\\n')\n   received = child.match.group(1).decode()\n\n   if received.strip() == test_input:\n       score = last_score + 1\n       status = \"success\"\n       print(\"Test PASSED ✓\")\n       print(f\"Score increased to: {score}\")\n   else:\n       score = 0\n       status = \"fail\"\n       print(\"Test FAILED ✗\")\n       print(f\"Score reset to: {score}\")\n       print(f\"Expected: '{test_input}'\")\n       print(f\"Got: '{received}'\")\n\n   with open('test_results.json', 'a') as f:\n       f.write(json.dumps({\n           \"score\": score,\n           \"status\": status,\n           \"source\": \"python_input_test\",\n           \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n       }) + \"\\n\")\n\n   print(f\"Results saved to test_results.json\")\n   print(f\"Final status: {status}\")\n\nfinally:\n   os.unlink(test_file)"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# python_hook_test\nimport json\nimport os\n\nfrom datetime import datetime\n\nprint(\"Starting Python hook test...\")\nprint(f\"Current directory: {os.getcwd()}\")\n\ntry:\n   with open('test_results.json', 'r') as f:\n       last_score = 0\n       for line in f:\n           try:\n               result = json.loads(line.strip())\n               if 'score' in result:\n                   last_score = int(result['score'])\n           except:\n               continue\nexcept (FileNotFoundError, ValueError):\n   last_score = 0\nprint(f\"Previous score: {last_score}\")\n\ntry:\n   if hook_python_test == \"Hello, World!\":\n       score = last_score + 1\n       status = \"success\"\n       print(\"Test PASSED ✓\")\n       print(f\"Score increased to: {score}\")\nexcept NameError:\n   score = 0\n   status = \"fail\"\n   print(\"Test FAILED ✗\")\n   print(f\"Score reset to: {score}\")\n   print(\"hook_python_test variable not found\")\n\nwith open('test_results.json', 'a') as f:\n   f.write(json.dumps({\n       \"score\": score,\n       \"status\": status,\n       \"source\": \"python_hook_test\",\n       \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n   }) + \"\\n\")\n\nprint(f\"Results saved to test_results.json\")\nprint(f\"Final status: {status}\")"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# bash_hook_test\necho \"Starting Bash hook test...\"\necho \"Current directory: $(pwd)\"\n\ncurrent_score=$(grep -o '\"score\": [0-9]*' test_results.json 2>/dev/null | tail -n1 | grep -o '[0-9]*' || echo 0)\necho \"Previous score: $current_score\"\n\n[ \"$hook_bash_test\" = \"Hello, World!\" ] && \\\n{ score=$((current_score + 1)); status=\"success\"; echo \"Test PASSED ✓\"; echo \"Score increased to: $score\"; } || \\\n{ score=0; status=\"fail\"; echo \"Test FAILED ✗\"; echo \"Score reset to: $score\"; } && \\\necho \"{\\\"score\\\": $score, \\\"status\\\": \\\"$status\\\", \\\"source\\\": \\\"bash_hook_test\\\", \\\"timestamp\\\": \\\"$(date \"+%Y-%m-%d %H:%M:%S\")\\\"}\" >> test_results.json && \\\necho \"Results saved to test_results.json\" && \\\necho \"Final status: $status\""
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "# npm_packages_test\necho \"Starting NPM packages test...\"\necho \"Current directory: $(pwd)\"\n\ncurrent_score=$(grep -o '\"score\": [0-9]*' test_results.json 2>/dev/null | tail -n1 | grep -o '[0-9]*' || echo 0)\necho \"Current score: $current_score\"\n\nORIGINAL_DIR=$(pwd)\ncd ../../web && npm list tailwindcss postcss autoprefixer > /dev/null 2>&1 && \\\n{ score=$((current_score + 1)); status=\"success\"; echo \"Test PASSED ✓\"; echo \"Score increased to: $score\"; } || \\\n{ score=0; status=\"fail\"; echo \"Test FAILED ✗\"; echo \"Score reset to: $score\"; } && \\\necho \"{\\\"score\\\": $score, \\\"status\\\": \\\"$status\\\", \\\"source\\\": \\\"npm_packages_test\\\", \\\"timestamp\\\": \\\"$(date \"+%Y-%m-%d %H:%M:%S\")\\\"}\" >> $ORIGINAL_DIR/test_results.json && \\\necho \"Results saved to test_results.json\" && \\\necho \"Final status: $status\"\n\ncd $ORIGINAL_DIR"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# sqlite_database_test\nimport json\nimport os\nimport sqlite3\n\nfrom datetime import datetime\n\nprint(\"Starting SQLite database test...\")\nprint(f\"Current directory: {os.getcwd()}\")\n\nEXPECTED_SCHEMAS = {\n    'flatpack_comments': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'block_id': 'TEXT NOT NULL',\n        'selected_text': 'TEXT NOT NULL',\n        'comment': 'TEXT NOT NULL',\n        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n    },\n    'flatpack_hooks': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'hook_name': 'TEXT NOT NULL',\n        'hook_placement': 'TEXT NOT NULL',\n        'hook_script': 'TEXT NOT NULL',\n        'hook_type': 'TEXT NOT NULL',\n        'show_on_frontpage': 'BOOLEAN DEFAULT 0',\n        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n    },\n    'flatpack_metadata': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'key': 'TEXT NOT NULL',\n        'value': 'TEXT NOT NULL'\n    },\n    'flatpack_schedule': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'type': 'TEXT NOT NULL',\n        'pattern': 'TEXT',\n        'datetimes': 'TEXT',\n        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',\n        'last_run': 'TIMESTAMP'\n    },\n    'flatpack_source_hook_mappings': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'source_id': 'TEXT NOT NULL',\n        'target_id': 'TEXT NOT NULL',\n        'source_type': 'TEXT NOT NULL',\n        'target_type': 'TEXT NOT NULL',\n        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n    },\n    'flatpack_sources': {\n        'id': 'INTEGER PRIMARY KEY AUTOINCREMENT',\n        'source_name': 'TEXT NOT NULL',\n        'source_type': 'TEXT NOT NULL',\n        'source_details': 'TEXT',\n        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'\n    }\n}\n\ndef get_table_info(cursor, table_name):\n    cursor.execute(f\"PRAGMA table_info({table_name})\")\n    columns = {}\n    for col in cursor.fetchall():\n        col_type = col[2]\n        if col[4]:\n            col_type += f\" DEFAULT {col[4]}\"\n        if col[3]:\n            col_type += \" NOT NULL\"\n        if col[5]:\n            col_type += \" PRIMARY KEY\"\n        columns[col[1]] = col_type.strip()\n    return columns\n\ndef is_schema_compatible(actual, expected):\n    if actual == expected:\n        return True\n    if \"INTEGER PRIMARY KEY\" in actual and \"INTEGER PRIMARY KEY AUTOINCREMENT\" in expected:\n        return True\n    return False\n\ntry:\n    with open('test_results.json', 'r') as f:\n        last_score = 0\n        for line in f:\n            try:\n                result = json.loads(line.strip())\n                if 'score' in result:\n                    last_score = int(result['score'])\n            except:\n                continue\nexcept (FileNotFoundError, ValueError):\n    last_score = 0\nprint(f\"Previous score: {last_score}\")\n\ntest_details = {\n    \"missing_tables\": [],\n    \"incorrect_schemas\": {},\n    \"found_tables\": []\n}\n\ntry:\n    db = sqlite3.connect('../../build/flatpack.db')\n    cursor = db.cursor()\n    \n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    tables = cursor.fetchall()\n    existing_tables = {t[0] for t in tables}\n    test_details[\"found_tables\"] = list(existing_tables)\n    \n    all_valid = True\n    \n    missing_tables = set(EXPECTED_SCHEMAS.keys()) - existing_tables\n    if missing_tables:\n        all_valid = False\n        test_details[\"missing_tables\"] = list(missing_tables)\n        print(f\"Missing tables: {missing_tables}\")\n    \n    for table in existing_tables:\n        if table in EXPECTED_SCHEMAS:\n            actual_schema = get_table_info(cursor, table)\n            expected_schema = EXPECTED_SCHEMAS[table]\n            \n            differences = {}\n            for col, expected_type in expected_schema.items():\n                if col not in actual_schema:\n                    differences[col] = f\"Missing column\"\n                elif not is_schema_compatible(actual_schema[col].upper(), expected_type.upper()):\n                    differences[col] = f\"Type mismatch. Expected: {expected_type}, Got: {actual_schema[col]}\"\n            \n            if differences:\n                all_valid = False\n                test_details[\"incorrect_schemas\"][table] = differences\n                print(f\"Schema mismatch in table {table}:\")\n                for col, error in differences.items():\n                    print(f\"  - {col}: {error}\")\n    \n    if all_valid:\n        score = last_score + 1\n        status = \"success\"\n        print(\"Test PASSED ✓\")\n        print(f\"Score increased to: {score}\")\n    else:\n        score = 0\n        status = \"fail\"\n        print(\"Test FAILED ✗\")\n        print(f\"Score reset to: {score}\")\n\nexcept Exception as e:\n    score = 0\n    status = \"fail\"\n    print(\"Test FAILED ✗\")\n    print(f\"Score reset to: {score}\")\n    print(f\"Database error: {str(e)}\")\n    test_details[\"error\"] = str(e)\nfinally:\n    if 'db' in locals():\n        db.close()\n\nwith open('test_results.json', 'a') as f:\n    f.write(json.dumps({\n        \"score\": score,\n        \"status\": status,\n        \"source\": \"sqlite_database_test\",\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"details\": test_details\n    }) + \"\\n\")\n\nprint(f\"Results saved to test_results.json\")\nprint(f\"Final status: {status}\")"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "# test_pass\nimport json\nimport os\n\nprint(\"Calculating final score...\")\nfinal_score = 0\nsuccess_count = 0\ntotal_count = 0\n\ntry:\n   with open('test_results.json', 'r') as f:\n       for line in f:\n           try:\n               result = json.loads(line.strip())\n               if result.get('status') == 'success':\n                   success_count += 1\n               total_count += 1\n               final_score = result.get('score', 0)\n           except json.JSONDecodeError:\n               continue\n\n   print(f\"Final score: {final_score}\")\n   print(f\"Success rate: {success_count}/{total_count} tests passed\")\n   if total_count > 0:\n       success_percentage = (success_count/total_count)*100\n       print(f\"Success percentage: {success_percentage:.2f}%\")\n       \n       if os.path.exists('test_pass'):\n           os.remove('test_pass')\n           print(\"Removed existing test_pass file\")\n\n       if success_percentage == 100:\n           with open('test_pass', 'w') as f:\n               f.write(f\"All tests passed: {success_count}/{total_count}\")\n           print(\"Created test_pass file\")\n\nexcept FileNotFoundError:\n   print(\"No test results file found\")\nexcept Exception as e:\n   print(f\"Error reading file: {e}\")"
  }
]