# flatpack.toml
# WIP ({{current_date}})

version = "0.0.1"

[environment]
model_name = "flatpack-test"

[[port]]
external = 1337
internal = 80

[directories]
checkpoints = "/home/content/checkpoints"
data = "/home/content/data"
models = "/home/content/models"
scripts = "/home/content/scripts"

[packages]

[packages.unix]
build-essential = "*"
gcc = "*"
git = "*"
python3-dev = "*"
python3-pip = "*"
wget = "*"

[packages.python]
datasets = "*"
numpy = "*"
tiktoken = "*"
transformers = "*"
torch = "*"
tqdm = "*"
wandb = "*"

[[git]]
from_source = "https://github.com/ggerganov/llama.cpp"
to_destination = "/home/content/llama.cpp"
branch = "master"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/flatpack-test/device.sh"
to_destination = "/home/content/device.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlingroup/flatpack-ai/main/warehouse/flatpack-test/build.sh"
to_destination = "/home/content/build.sh"

[[run]]
command = "chmod"
args = "+x /home/content/device.sh"

[[run]]
command = "chmod"
args = "+x /home/content/build.sh"

[[cmd]]
command = "bash"
args = "/home/content/build.sh"