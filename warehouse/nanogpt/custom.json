[
  {
    "type": "bash",
    "disabled": false,
    "code": "../bin/pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "../bin/python data/shakespeare_char/prepare.py"
  },
  {
    "type": "bash",
    "disabled": false,
    "code": "../bin/python train.py config/train_shakespeare_char.py \\\n    --device=cpu \\\n    --compile=False \\\n    --eval_iters=20 \\\n    --log_interval=1 \\\n    --block_size=64 \\\n    --batch_size=12 \\\n    --n_layer=4 \\\n    --n_head=4 \\\n    --n_embd=128 \\\n    --max_iters=2000 \\\n    --lr_decay_iters=2000 \\\n    --dropout=0.0"
  },
  {
    "type": "python",
    "disabled": false,
    "code": "import os\nimport pickle\nimport tiktoken\nimport torch\n\nfrom contextlib import nullcontext\nfrom model import GPTConfig, GPT\n\ninit_from = 'resume'\nout_dir = 'out-shakespeare-char'\nstart = \"\\n\"\nnum_samples = 10\nmax_new_tokens = 500\ntemperature = 0.8\ntop_k = 200\nseed = 1337\ndevice = 'cpu'\ndtype = 'float16'\ncompile = False\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\nif init_from == 'resume':\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n    gptconf = GPTConfig(**checkpoint['model_args'])\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    unwanted_prefix = '_orig_mod.'\n    \n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\nelif init_from.startswith('gpt2'):\n    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n\nmodel.eval()\nmodel.to(device)\n\nif compile:\n    model = torch.compile(model)\n\nload_meta = False\n\nif init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']:\n    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n    load_meta = os.path.exists(meta_path)\n\nif load_meta:\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    stoi, itos = meta['stoi'], meta['itos']\n    encode = lambda s: [stoi[c] for c in s]\n    decode = lambda l: ''.join([itos[i] for i in l])\nelse:\n    enc = tiktoken.get_encoding(\"gpt2\")\n    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n    decode = lambda l: enc.decode(l)\n\nif start.startswith('FILE:'):\n    with open(start[5:], 'r', encoding='utf-8') as f:\n        start = f.read()\nstart_ids = encode(start)\nx = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n\nwith torch.no_grad():\n    with ctx:\n        with open('output.txt', 'w', encoding='utf-8') as f:\n            for k in range(num_samples):\n                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n                output_text = decode(y[0].tolist())\n                f.write(output_text)"
  }
]