import argparse
import mss
import signal
import time
import torch

from PIL import Image
from threading import Thread
from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM

try:
    from moondream import detect_device, LATEST_REVISION
except ImportError:
    # Define a custom detect_device function if import fails
    def detect_device():
        """
        Detect the appropriate device (CUDA, MPS, or CPU).
        Returns:
            tuple: A tuple containing the device and the corresponding data type.
        """
        if torch.cuda.is_available():
            return torch.device("cuda"), torch.float16
        if torch.backends.mps.is_available():
            return torch.device("mps"), torch.float16
        return torch.device("cpu"), torch.float32


    LATEST_REVISION = "main"

# Argument parser for CPU option
parser = argparse.ArgumentParser()
parser.add_argument("--cpu", action="store_true")
args = parser.parse_args()

# Determine device
if args.cpu:
    device = torch.device("cpu")
    dtype = torch.float32
else:
    device, dtype = detect_device()
    if device != torch.device("cpu"):
        print("Using device:", device)
        print("If you run into issues, pass the `--cpu` flag to this script.")
        print()

# Load model and tokenizer
model_id = "vikhyatk/moondream2"
tokenizer = AutoTokenizer.from_pretrained(model_id, revision=LATEST_REVISION)
moondream = AutoModelForCausalLM.from_pretrained(
    model_id, trust_remote_code=True, revision=LATEST_REVISION
).to(device=device, dtype=dtype)
moondream.eval()


def cleanup():
    """
    Perform any necessary cleanup.
    """
    print("Cleanup complete.")


def signal_handler(sig, frame):
    """
    Handle termination signals to perform cleanup.

    Args:
        sig (int): The signal number.
        frame (frame): The current stack frame.
    """
    cleanup()
    exit(0)


# Register signal handler for graceful termination
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


def answer_question(img, prompt):
    """
    Generate an answer to a question based on the provided image.

    Args:
        img (PIL.Image.Image): The image in PIL format.
        prompt (str): The question prompt to be answered.

    Returns:
        str: The response generated by the model.
    """
    image_embeds = moondream.encode_image(img)
    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
    thread = Thread(
        target=moondream.answer_question,
        kwargs={
            "image_embeds": image_embeds,
            "question": prompt,
            "tokenizer": tokenizer,
            "streamer": streamer,
        },
    )
    thread.start()
    thread.join()  # Wait for the thread to complete

    buffer = ''.join(streamer)
    return buffer.strip()


def main():
    """Main function to capture screenshots."""
    frame_count = 0
    prompt = "What's going on? Respond with a single sentence."

    with mss.mss() as sct:
        monitor = sct.monitors[0]  # Use the first monitor

        try:
            while True:
                screenshot = sct.grab(monitor)
                img = Image.frombytes("RGB", (screenshot.width, screenshot.height), screenshot.rgb)

                frame_count += 1
                width, height = img.size
                print(f"Captured frame {frame_count}: resolution {width}x{height}")

                response = answer_question(img, prompt)
                print(f"Moondream analysis: {response}")

                time.sleep(5)  # Capture one frame every fifth second
        finally:
            cleanup()


if __name__ == "__main__":
    main()
