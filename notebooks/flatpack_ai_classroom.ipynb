{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+LQc0+rwszgGBzuuFHM6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romlingroup/flatpack-ai/blob/main/notebooks/flatpack_ai_classroom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# flatpack.ai - Classroom"
      ],
      "metadata": {
        "id": "BR2y05K4tdST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "OdrbuNF2Tuzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge distillation"
      ],
      "metadata": {
        "id": "1ZBACZY3aAbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9xnLtvYtSCl"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "file_path = \"/content/input.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    !wget https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 1024\n",
        "chunk_size = sequence_length - 1\n",
        "num_chunks = len(shakespeare_text) // chunk_size\n",
        "\n",
        "shakespeare_tokens = torch.zeros([num_chunks, sequence_length], dtype=torch.long, device=device)\n",
        "for i in range(num_chunks):\n",
        "    chunk = shakespeare_text[i * chunk_size:(i + 1) * chunk_size]\n",
        "    tokens = tokenizer(chunk, return_tensors=\"pt\", padding='max_length', max_length=sequence_length)[\"input_ids\"]\n",
        "    shakespeare_tokens[i] = tokens[0]\n",
        "\n",
        "num_batches = len(shakespeare_tokens) // batch_size\n",
        "\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "temperature = 2.0\n",
        "continue_training = True\n",
        "\n",
        "checkpoint_dir = \"/content/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "if continue_training:\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_3.pth\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    last_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    last_epoch = -1\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "for epoch in range(last_epoch + 1, num_epochs):\n",
        "    print(f\"Epoch {epoch+1} started.\")\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        batch_tokens = shakespeare_tokens[start_idx:end_idx]\n",
        "\n",
        "        teacher_outputs = teacher_model(batch_tokens, return_dict=True)\n",
        "        student_outputs = student_model(batch_tokens, return_dict=True)\n",
        "\n",
        "        distill_loss = F.kl_div(\n",
        "            F.log_softmax(student_outputs.logits / temperature, dim=-1),\n",
        "            F.softmax(teacher_outputs.logits / temperature, dim=-1),\n",
        "            reduction='batchmean'\n",
        "        )\n",
        "\n",
        "        original_loss = F.cross_entropy(student_outputs.logits[:, :-1].contiguous().view(-1, student_outputs.logits.size(-1)), batch_tokens[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        loss = distill_loss + original_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{num_batches}, Loss: {loss.item()}\")\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': student_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss.item(),\n",
        "    }, checkpoint_path)\n",
        "\n",
        "if 'shakespeare_text' in locals():\n",
        "    del shakespeare_text\n",
        "if 'shakespeare_tokens' in locals():\n",
        "    del shakespeare_tokens\n",
        "if 'teacher_outputs' in locals():\n",
        "    del teacher_outputs\n",
        "if 'student_outputs' in locals():\n",
        "    del student_outputs\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference from checkpoint"
      ],
      "metadata": {
        "id": "gCfcp2x3Z89h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "checkpoint_dir = \"/content/checkpoints\"\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_6.pth\")\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "student_model = student_model.to(device)\n",
        "student_model.eval()\n",
        "\n",
        "sequence_length = 1024\n",
        "\n",
        "input_text = \"Once upon a time\"\n",
        "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_tokens = student_model.generate(\n",
        "        input_tokens,\n",
        "        max_length=150,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=2.0,\n",
        "        top_k=40,\n",
        "        top_p=0.90,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "gZxe2qRGZEyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload model to Hugging Face"
      ],
      "metadata": {
        "id": "GrzA1UeWzPVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "e8QXqhb8xffu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = \"/content/student_model\"\n",
        "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "    !rm -r $folder_path"
      ],
      "metadata": {
        "id": "3-gkwSsz7j6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = folder_path\n",
        "student_model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "9nyxv2e8zeNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "user = \"romlingroup\"\n",
        "model_repo_name = \"gpt2-shakespeare-student\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "whoami = api.whoami(token=api.token)\n",
        "print(f\"Logged in as user: {whoami['name']}\")\n",
        "\n",
        "repo_id = f\"{user}/{model_repo_name}\"\n",
        "api.create_repo(token=api.token, repo_id=repo_id, private=False, exist_ok=True)"
      ],
      "metadata": {
        "id": "XzoAGPtXx24A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = f\"{user}/{model_repo_name}\"\n",
        "api.upload_folder(\n",
        "    folder_path=save_directory,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "pR09PqCTzYfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "temp_dir = \"/content/temp_upload_dir\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "shutil.move(checkpoint_dir, os.path.join(temp_dir, \"checkpoints\"))\n",
        "\n",
        "repo_id = f\"{user}/{model_repo_name}\"\n",
        "api.upload_folder(\n",
        "    folder_path=temp_dir,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "shutil.move(os.path.join(temp_dir, \"checkpoints\"), checkpoint_dir)\n",
        "os.rmdir(temp_dir)"
      ],
      "metadata": {
        "id": "gmFIXhMJ89Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}